{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "Everything stored in the <span style=\"background-color: #d5d8dc \">/home/datascience</span> folder is now stored on your block volume drive. The <span style=\"background-color: #d5d8dc \">ads-examples</span> folder has moved outside of your working space and is now made available through a symbolic link to <span style=\"background-color: #d5d8dc \">ads-examples</span> (found at <span style=\"background-color: #d5d8dc \">/home/datascience/ads-examples</span>.)\n",
    "<details>\n",
    "<summary><font size=\"2\">1. Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">2. OCI Configuration and Key Files Set Up</font></summary><p>Follow instructions in <span style=\"background-color: #d5d8dc \">getting-started.ipynb</span> (located in the home folder)</p>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">3. Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "<li>Notebook Examples can be found in the <span style=\"background-color: #d5d8dc \">ads-examples</span> directory.</li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">4. Typical Cell Imports and Settings</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import MLData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">5. Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend\n",
    "%pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import tempfile\n",
    "import warnings\n",
    "from ads.dataset.dataset_browser import DatasetBrowser\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from dask.datasets import timeseries\n",
    "from os import path\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "ads.set_documentation_mode(False)\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='src'></a>\n",
    "## Loading Datasets From Various Sources\n",
    "\n",
    "Loading data into ADS can be done in several different ways. Data can load from a local, network file system, Hadoop Distributed File System (HDFS), Oracle Object Storage, Amazon S3, Google Cloud Service, Azure Blob, Oracle DB, ADW, elastic search instance, NoSQL DB instance, Mongodb and many more sources. This notebook demonstrates how to do this for some of the more common data sources. However, the approach is generalizable to the other data sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adb'></a>\n",
    "### Oracle Autonomous Database (ADB)\n",
    "\n",
    "The Autonomous Database (ADB) is a cloud-based database that has minimal administration requirements. There are two different configurations that are optimized for different use cases. The Autonomous Data Warehouse (ADW) and the Autonomous Transaction Processing (ATP) databases. Once the security credential configuration has been set up, an `ADSDataset` can be obtained just like any other file that is supported by the `DatasetFactory.open()` method.\n",
    "\n",
    "ADB credentials and connection information is provided in two parts. The first part comes from the ADB Wallet file. The `TNS_ADMIN` environment variable must be specified to put at `sqlnet.ora` file in the Wallet directory. In addition, a URI must be defined. The protocol used is the database type plus the driver type. Specifically, this would be `oracle+cx_oracle`. The URI also includes the username and password along with the ADB consumer group (SID). The URI would look something like the following\n",
    "\n",
    "```\n",
    "oracle+cx_oracle://admin:mypassword@mydatabase_medium'\n",
    "```\n",
    "\n",
    "In the `DatasetFactory.open()` method, there is a parameter `table` that can list a table that is to be returned or it can be a Data Query Language (DQL) command, such as SELECT, that returns a set of records. The `format='sql'` setting lets the method know that the connection will be to a database.\n",
    "\n",
    "There is a notebook that details how to set up a connection to the Autonomous Database (ADB). If that connection is already configured, the following code can be run to test a connection. Please update the connection information before executing the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TNS_ADMIN=/home/datascience/wallet\n",
    "%env ADW_SID=db202109270858_low\n",
    "%env ADW_USER=OMLUSER\n",
    "%env ADW_PASSWORD=Welcome12345\n",
    "\n",
    "!echo exit | sqlplus ${ADW_USER}/${ADW_PASSWORD}@${ADW_SID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NLS_LANG\"] = \"AMERICAN\"\n",
    "from sqlalchemy import create_engine\n",
    "uri=f'oracle+cx_oracle://{os.environ[\"ADW_USER\"]}:{os.environ[\"ADW_PASSWORD\"]}@{os.environ[\"ADW_SID\"]}'\n",
    "engine=create_engine(uri,max_identifier_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas=pd.read_sql_query('SELECT * FROM ONLINE_RETAIL', con=engine)\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='local'></a>\n",
    "### Local File Storage\n",
    "\n",
    "Files that are stored locally in the notebook environment can also be read with the same command. The notebook environment provides a number of sample datasets in the `/opt/notebooks/ads-examples/oracle_data` and `/opt/notebooks/ads-examples/3P_data` directory. `DatasetFactory.open()` understands a number of file extensions and will make best efforts to set the parameters needed to read the file. This decreases workload and reduces the number of coding errors.\n",
    "\n",
    "In the example below, reading from a CSV file is demonstrated. However, `DatasetFactory.open()` can read from a variety of file formats. See the section <a href='#fileformat'>Loading datasets of various file formats</a> for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"/home/datascience/Online_Retail.csv\"\n",
    "df_pandas=pd.read_csv(df_path ,encoding='ISO-8859-1') \n",
    "df_pandas.columns = map(str.lower, df_pandas.columns)\n",
    "print(df_pandas.shape)\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform plots of the top 10 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prdcnt_df=df_pandas.groupby('description')['stockcode'].count().sort_values(ascending=False).to_frame()\n",
    "prdcnt_df['percent']=prdcnt_df['stockcode']/sum(prdcnt_df['stockcode'])*100\n",
    "prdcnt_df_long = prdcnt_df\n",
    "prdcnt_df=prdcnt_df.head(10)\n",
    "prdcnt_df['percent'].plot(kind='bar',label=True)\n",
    "plt.title(\"Top 10 products\")\n",
    "plt.xlabel(\"Product\")\n",
    "plt.ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prdcnt_df_long = prdcnt_df_long[prdcnt_df_long.percent >= 0.2]\n",
    "prdcnt_df_long = prdcnt_df_long.drop(columns='stockcode')\n",
    "df_pandas = df_pandas.merge(prdcnt_df_long, on='description')\n",
    "\n",
    "print(df_pandas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crosstab the results in preparation for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets_Category=pd.crosstab(index=df_pandas['customerid'], columns=df_pandas['stockcode'], values=df_pandas['quantity'], margins=True,  aggfunc=\"sum\").fillna(0).astype('int')\n",
    "print(baskets_Category.shape)\n",
    "baskets_Category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace quantities with purchase or not purchase indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets_Category = baskets_Category.iloc[:, :-1]\n",
    "baskets_Category_binary = baskets_Category.copy()\n",
    "baskets_Category_binary[baskets_Category_binary > 0] = 1\n",
    "baskets_Category_binary[baskets_Category_binary<=0] =0\n",
    "baskets_Category_binary[pd.isna(baskets_Category_binary)] =0\n",
    "baskets_Category_binary[pd.isnull(baskets_Category_binary)] =0\n",
    "print(baskets_Category_binary.shape)\n",
    "baskets_Category_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Apriori algorithm\n",
    "Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.\n",
    "    Output metrics of the algorithm are:     \n",
    "* Measure 1: Support. This says how popular an itemset is, as measured by the proportion of transactions in which an itemset\n",
    "    appears. If an item is purchased in 4 out of 8 transactions, then the support is  50%. \n",
    "* Measure 2: Confidence. This says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is\n",
    "    measured by the proportion of transactions with item X, in which item Y also appears. If beers are purchased 3 times out\n",
    "    of 4 transctions where apples are purchased, then the confidence is 3 out of 4, or 75%.\n",
    "* Measure 3: Lift. This says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y\n",
    "    is. A lift value of 1,which implies no association between items. A lift value greater than 1 means that item Y is likely\n",
    "    to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is   \n",
    "    bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply thresholds on support\n",
    "baskets_Category_binary_itemsets = apriori(baskets_Category_binary, min_support=.0075, use_colnames=True)\n",
    "baskets_Category_binary_itemsets['length'] = baskets_Category_binary_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "baskets_Category_binary_itemsets = baskets_Category_binary_itemsets.sort_values(by=['support'],ascending=False)\n",
    "print(baskets_Category_binary_itemsets.shape)\n",
    "baskets_Category_binary_itemsets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply thresholds on lift\n",
    "\n",
    "rules_Category= association_rules(baskets_Category_binary_itemsets, metric=\"lift\", min_threshold=1.1)\n",
    "rules_Category[\"antecedents2\"] = rules_Category[\"antecedents\"].apply(lambda x: list(x)[0]).astype(\"unicode\")\n",
    "rules_Category[\"consequents2\"] = rules_Category[\"consequents\"].apply(lambda x: list(x)[0]).astype(\"unicode\")\n",
    "rules_Category['length'] = rules_Category['antecedents'].apply(lambda x: len(x))\n",
    "rules_Category=rules_Category[ (rules_Category['length'] == 1)]\n",
    "rules_Category = pd.DataFrame(rules_Category) \n",
    "del rules_Category['antecedents']\n",
    "del rules_Category['consequents']\n",
    "rules_Category['antecedents']=rules_Category['antecedents2']\n",
    "rules_Category['consequents']=rules_Category['consequents2']\n",
    "del rules_Category['antecedents2']\n",
    "del rules_Category['consequents2']\n",
    "print(rules_Category.shape)\n",
    "rules_Category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform data wrangling\n",
    "\n",
    "Category_rules=rules_Category.groupby(['antecedents','consequents'])['antecedent support','consequent support','support','confidence','lift','leverage','conviction'].max()\n",
    "rules_Category=rules_Category.sort_values(by=['support'],ascending=False)\n",
    "print(rules_Category.shape)\n",
    "rules_Category.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining association rules often results in a very large number of found rules, leaving the analyst with the task to go through all the rules and discover interesting ones. Shifting manually through large sets of rules is time consuming and strenuous. To overcome this, we use Networkx to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from pyvis.network import Network\n",
    "import numpy as np \n",
    "import pylab as plt \n",
    "from itertools import count \n",
    "from operator import itemgetter \n",
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision',10)\n",
    "G = nx.from_pandas_edgelist(rules_Category, source='antecedents', target='consequents', edge_attr=[\"support\"],create_using = nx.Graph())\n",
    "\n",
    "nodes = G.nodes()\n",
    "degree = G.degree()\n",
    "colors = [degree[n] for n in nodes]\n",
    "\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "pos=nx.fruchterman_reingold_layout(G)\n",
    "\n",
    "cmap = plt.cm.viridis_r\n",
    "\n",
    "vmin = min(colors)\n",
    "vmax = max(colors)\n",
    "\n",
    "fig = plt.figure(figsize = (15,9), dpi=100)\n",
    "\n",
    "results_f=rules_Category.loc[rules_Category['lift']>=1]\n",
    "members=[antecedents for antecedents in list(results_f['antecedents']) ]\n",
    "\n",
    "SITEs=[consequents for consequents in list(results_f['consequents']) ]\n",
    "\n",
    "min1=min(results_f['support'])\n",
    "max1=max(results_f['support'])\n",
    "support=[ ((support -min1)/(max1-min1))*100+50 for support in list(results_f['support']) ]\n",
    "\n",
    "nx.draw(G,pos,alpha = 0.8, nodelist = SITEs, node_color = 'lightblue', node_size = support, font_size = 10, width = 0.2, cmap = cmap, edge_color ='red',node_shape='d',with_labels = True)\n",
    "\n",
    "\n",
    "fig.set_facecolor('#faf7f8')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#black are members\n",
    "#light blue are sites are members\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaberative filtering\n",
    "\n",
    "Recommender systems are an important class of machine learning algorithms that offer “relevant” suggestions to users. Youtube, Amazon, Netflix, all function on recommendation systems where the system recommends you the next video or product based on your past activity (Content-based Filtering) or based on activities and preferences of other users similar to you (Collaborative Filtering)\n",
    "Recommendation Systems work based on the similarity between either the content or the users who access the content.\n",
    "There are several ways to measure the similarity between two items. The recommendation systems use this similarity matrix to recommend the next most similar product to another product, based on purchase patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_summary=df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a table of products purchased by each custome\n",
    "\n",
    "baskets=pd.crosstab(index=merged_df_summary['customerid'], columns=merged_df_summary['description'], values=merged_df_summary['quantity'], margins=True,  aggfunc=\"sum\").fillna(0).astype('int')\n",
    "\n",
    "#get rid of last column since that is a summation\n",
    "baskets = baskets.iloc[:, :-1]\n",
    "\n",
    "#if there is a purchase, then it is a 1, otherwise 0\n",
    "baskets_binary = baskets.copy()\n",
    "baskets_binary[baskets_binary > 0] = 1\n",
    "baskets_binary[baskets_binary<=0] =0\n",
    "baskets_binary[pd.isna(baskets_binary)] =0\n",
    "baskets_binary[pd.isnull(baskets_binary)] =0\n",
    "\n",
    "#drops rows with all 0's\n",
    "baskets_binary=baskets_binary.loc[~(baskets_binary==0).all(axis=1)]\n",
    "\n",
    "#drops columns with all 0's\n",
    "baskets_binary=baskets_binary.loc[:, (baskets_binary != 0).any(axis=0)]\n",
    "print(baskets_binary.shape)\n",
    "baskets_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a table of products compared to each other product (by purchase patterns) that can be used to create similarity matrixes\n",
    "\n",
    "temp=pd.DataFrame(index=baskets_binary.columns,columns=baskets_binary.columns)\n",
    "\n",
    "#This loops through each product and calculates the cosine similarity between each other product\n",
    "\n",
    "print('total square Matrix size',len(baskets_binary.columns))\n",
    "for i in range(0, len(baskets_binary.columns) ):\n",
    "    for j in range(0,len(baskets_binary.columns)):\n",
    "        temp.iloc[i,j]=1-cosine(baskets_binary.iloc[:,i],baskets_binary.iloc[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save/Read the output file\n",
    "\n",
    "temp = temp.loc[:, ~temp.columns.str.contains('^Unnamed')]\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathers Metadata\n",
    "\n",
    "no_products = len(temp.columns)\n",
    "print('Number of product in dataset:', no_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table of most recommened products, ranked on Cosine Similarity\n",
    "\n",
    "a=pd.DataFrame(temp.stack())  \n",
    "a.reset_index(level=0, inplace=True)\n",
    "a['rec_product']=a.index \n",
    "a.reset_index(drop=True, inplace=True) \n",
    "a.columns = ['original_product', 'cosine_sim','reccommended_product']\n",
    "a = a[a['original_product']!=a['reccommended_product']]\n",
    "a = a[a['cosine_sim']!=1]\n",
    "a=a.sort_values(by=['original_product','cosine_sim'],ascending=False)\n",
    "a['cosine_sim']=pd.to_numeric(a['cosine_sim']  )\n",
    "b=a.groupby(['original_product']).apply(lambda grp: grp.nlargest(3, 'cosine_sim'))\n",
    "b.reset_index(drop=True, inplace=True) \n",
    "b['rank'] = b.sort_values(['original_product','cosine_sim'], ascending=[True,False]) \\\n",
    "             .groupby(['original_product']) \\\n",
    "             .cumcount() + 1\n",
    "b[['rank','original_product','reccommended_product']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Here we build a very simple model that reccmoends the top product based on the original product (data frame is in descending order). Afterwards, we will deploy the model to the OCI model catalog. Here it is imperative that you have the correct policies in place to give you access to the model catalog. Check out this link to ensure that you have the correct policies in place https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(product):\n",
    "    print(product)\n",
    "    for index, row in b.iterrows():\n",
    "        if row['original_product'] == product:\n",
    "            return row['reccommended_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os\n",
    "from os import path\n",
    "from joblib import dump\n",
    "import cloudpickle\n",
    "ads.set_auth(auth='resource_principal')\n",
    "path_to_model_artifacts = \"online-retail\"\n",
    "generic_model_artifact = prepare_generic_model(\n",
    "    path_to_model_artifacts,\n",
    "    force_overwrite=True,\n",
    "    function_artifacts=False,\n",
    "    data_science_env=True)\n",
    "with open(path.join(path_to_model_artifacts, \"model.pkl\"), \"wb\") as outfile: cloudpickle.dump(model, outfile)\n",
    "catalog_entry = generic_model_artifact.save(display_name='online-retail-model',\n",
    "        description='Model to reccommend online retaiL products')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlcpuv1]",
   "language": "python",
   "name": "conda-env-mlcpuv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
